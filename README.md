# American Express-Default Prediction

---

프로젝트 수행 이력: 2021.09.01 ~ 2021.12.31

*2024.01.28 포트폴리오 목적으로  Readme 작성 후 업로드: 조문근*

- [American Express-Default Prediction](#american-express-default-prediction)
  * [Abstract](#abstract)
  * [1 개요](#1---)
    + [1.1 프로젝트의 특징과 기대되는 성과](#11------------------)
  * [2 문제 정의](#2------)
    + [2.1 성능의 평가](#21-------)
  * [3 데이터 전처리](#3--------)
    + [3.1 데이터 입출력](#31--------)
    + [3.2 정규화 및 표준화, Feature Engineering](#32------------feature-engineering)
  * [4 LightGBM](#4-lightgbm)
  * [5 결과](#5---)
  * [Reference](#reference)

## 핵심기여(Key Contributions)

본 문서는 본인이 2021년 2학기 건국대학교 SW중심대학 장학활동으로 수행하였던 프로젝트의 요약본임. 

대용량 데이터 경량화 및 처리
- 정밀도 축소: 60GB(fp64)의 원본 데이터를 fp16으로 경량화하여, 학습 데이터 기준 16.4GB를 1.8GB로 압축. (3.1절)
- 속도-성능 Trade-off: 병렬 프로그래밍(multiprocessing)을 적용, 3시간 이상 소요되던 데이터 로드 시간을 30분 이내로 단축하며 성능(Accuracy) 저하가 없음을 검증.

## 1 개요

### 1.1 프로젝트의 특징과 기대되는 성과

본 프로젝트는 Kaggle 플랫폼을 이용하고 American Express에서 주최한 AI 경진대회를 수행하며, 수십억 데이터포인트를 가지고 있는 대규모 데이터셋에서의 머신러닝 과제를 수행하는 것임. 1) 오로지 성능만을 평가한다는 것과 리더보드에 2) 타 팀의 코드를 참조할 수 있다는 점에서 팀원들의 개별적인 모델링 역량 향상에 많은 도움이 될 것으로 기대됨.

## 2 문제 정의

Kaggle Competition이라, 풀고자하는 문제가 명확하게 정의되어 있었음. 카드사에서 수집한 고객들의 신용카드 내역 데이터로, 익월의 카드 대금 미납 확률을 예측해야함. 학습 데이터는 수십만명의 300개의 컬럼데이터임. 또한 개인정보 보호 이슈로 컬럼이 무엇을 뜻하는지 알 수 없으며, EDA를 통한 추론 방지를 위해 정규화 및 표준화가 되어 있는 상태임.

### 2.1 성능의 평가

$D$는 예측한 미납 가능성의 상위 4%의 F1 Score임. $G$는 정규화된 지니계수임. 위 두 가지 지표의 평균값이 최종 성능 지표임.

$$
M=0.5\cdot (G+D)
$$

## 3 데이터 전처리

### 3.1 데이터 입출력

▶ **정밀도 축소를 통한 시공간 복잡도 절약**

데이터 로드에만 3시간 이상, 이후 LGBM을 이용한 학습에는 24시간 이상이 소모되는 이슈 발생하였음. 원인은 데이터 정밀도가 fp64로, 가용한 인프라에서는 부담스러운 정밀도 였음. 이에 따라 정밀도 축소를 사용하여 문제 해결하였음. 수치 데이터는 fp16로 변환, categorical data는 그대로 사용하였음.

| Data | 데이터포인트 | .csv 크기 | .feather 크기 |
| --- | --- | --- | --- |
| train_data | 1억3170만 개 | 16.4 GB | 1.8 GB |
| test_data | 2억8700만 개 | 33.8 GB | 3.6 GB |

▶ **병렬 프로그래밍을 통한 데이터 입출력 최적화**

효율적인 자원 사용을 위해, 수십만개의 Row를 가지고 있는 데이터프레임을 8분할 한 다음 각각 다른 CPU를 사용하여 메모리에 적재 후 통합하였음. 총 데이터 로드 시간이 30분 이내로 단축(90% 향상)됨.

### 3.2 정규화 및 표준화, Feature Engineering

카드 사용 데이터라, 고객 익명성 보장을 위해 정규화 및 표준화가 모두 적용된 상태였음. 또한 해당 수치가 무엇을 뜻하는지 알 수 없음. 열의 이름이 D_91, B_36등으로 무엇을 뜻하는 지 알 수 없게 마스킹되어있음. 이에 따라 추가적인 Feature Engineering이 불가능함. 또한 평균, 분산, 왜도, 첨도 등의 수치는 성능에 유의미한 변화를 일으키지 못하였음. 또한 카테고리 데이터의 경우 Gradient Boosting 모델의 경우 자동적으로 원-핫 인코딩 되므로 별 다른 조치를 취하지 않았음.

## 4 LightGBM

▶ **모델 선택과 손실함수**

1) 정형 데이터를 사용한 알고리즘 중 회귀, 분류 문제에서 높은 성능을 보이는 Gradient Boost, 2) 그 중 학습 속도가 가장 우수한 점을 고려하여 LightGBM을 사용하였음. 학습데이터는 미납(1) 혹은 정상납부(0)로 레이블링 되어있고, 미납 확률을 예측하는 회귀 문제이므로 binary logloss 손실함수를 사용하였음.

▶ **불균형 데이터에 대한 하이퍼 파라미터: scale_pos_weight**

미납의 경우가 아닌 경우에 비해 적을 것으로 예상됨. 이에 따라 해당 경우의 손실 가중치를 2배로 설정하였음. 

## 5 결과

0.80의 성능 결과를 보여주었음. 1등의 확률이 0.80485..인 것과 확인할 수 있는 리더 보드도 모두 0.8에서 소수점 아래 셋째자리 경쟁인 것과 '신뢰성'과 '안정성'을 고려할 때 유의미하다고 판단됨. 이 이상의 성능 향상을 위한 시도는 1) 리소스 인풋 대비 아웃풋 2) 채점 데이터에 대한 과적합 측면을 고려하여 부적절하다고 판단하였음.


## Reference 
▶ **LightGBM**

[1] Guolin Ke et al., “LightGBM: A Highly Efficient Gradient Boosting Decision Tree”, NIPS 2017

▶ **데이터 병렬 입출력**

[2] “multiprocessing - Process-based parallelism”, Python3 Documentation, https://docs.python.org/3/library/multiprocessing.

▶ **정밀도 축소를 통한 성능 향상의 아이디어 도출**

[3] Xiao Sun et al., “Ultra-Low Precision 4-bit Training of Deep Neural Networks”, NIPS 2020
